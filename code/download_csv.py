import requests
import re
import gzip
import os
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from enums import SensorModel
from tqdm import tqdm


# Folder where csv files are stored
DOWNLOAD_FOLDER = "sensor_community_archive/csv"
# url from where to download data
URL = "https://archive.sensor.community/"
# regex pattern for a day
PATTERN_DAY = r"\d\d\d\d-\d\d-\d\d/"
# regex pattern for a year 
PATTERN_YEAR = r"\d\d\d\d/"
# a list of all url of csv files that should be imported
# the list was generated by calling list_website(URL)
DOWNLOAD_LIST = "sensor_community_archive/download_list.txt"
# shows how many files have been downloaded
PROGRESS_FILE = "sensor_community_archive/progress.txt"
# file where logs are saved
LOG_FILE = "sensor_community_archive/log.txt"
all_csv_urls = []


def log(*l):
    """
    simple logging 
    """
    with open(LOG_FILE, "a") as f:
        print(' '.join(str(x) for x in l), file=f)


def download(url, trys = 5):
    """
    Downloads a file from the given URL, extracts if .csv.gz, and saves as .csv.
    """
    csv_filename = url.split("/")[-1].removesuffix('.gz')
    raw = None
    for _ in range(trys):
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            raw = response.raw
            break
        except Exception:
            continue

    if not raw:
        log(f'Faild to download File: {csv_filename}')
        return
    
    with open(os.path.join(DOWNLOAD_FOLDER, csv_filename), 'wb') as csv_file:
        file_content = gzip.GzipFile(fileobj=raw) if url.endswith('.gz') else raw
        csv_file.write(file_content.read())

    return csv_filename


def list_website(url, trys = 5):
    """
    recursively finds all csv files and saves them to download_list.txt 
    """

    page = None

    for _ in range(trys):
        try:
            response = requests.get(url)
            response.raise_for_status()
            page = response.text
            break
        except Exception:
            continue

    if not page:
        log(f'Faild to list: {url}')
        return

    soup = BeautifulSoup(page, "html.parser")
    # walk into all months
    for item in reversed(soup.find_all("a")):
        link = item.get("href")
        if re.fullmatch(PATTERN_DAY, link):
            list_website(urljoin(url, link)) 
        if re.fullmatch(PATTERN_YEAR, link):
            list_website(urljoin(url, link))
        if link.endswith(".csv") or link.endswith(".gz"):
            for sensor_name in SensorModel._names.values():
                if sensor_name.lower() in link:
                    break
            else:
                continue
            all_csv_urls.append(urljoin(url, link))
            print(urljoin(url, link), file=open(DOWNLOAD_LIST, 'a'))


def get_urls():
    """
    generator to yiled urls from download list 
    """
    with open(DOWNLOAD_LIST, "r") as f:
        while (url := f.readline()):
            yield url.strip()


def main():
    for url in tqdm(get_urls(), desc="Downloading files", unit="files", file=open(PROGRESS_FILE, "w")):
        download(url)


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        log(e) 
