import requests
import re
import gzip
import os
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from enums import SensorModel
from tqdm import tqdm
from database import get_db
from models import Station


# Folder where csv files are stored
DOWNLOAD_FOLDER = "sensor_community_archive/csv"
# url from where to download data
URL = "https://archive.sensor.community/"
# regex pattern for a day
PATTERN_DAY = r"\d\d\d\d-\d\d-\d\d/"
# regex pattern for a year 
PATTERN_YEAR = r"\d\d\d\d/"
#
PATTERN_STATION_ID = r"sensor_(\d+)"
# a list of all url of csv files that should be imported
# the list was generated by calling list_website(URL)
DOWNLOAD_LIST = "sensor_community_archive/download_list.txt"
# shows how many files have been downloaded
PROGRESS_FILE = "sensor_community_archive/progress.txt"
# file where logs are saved
LOG_FILE = "sensor_community_archive/log.txt"

all_csv_urls = []
log_file = None


def log(*l):
    """
    simple logging 
    """
    print(' '.join(str(x) for x in l), file=log_file)


def download(url, trys = 5):
    """
    Downloads a file from the given URL, extracts if .csv.gz, and saves as .csv.
    """
    csv_filename = url.split("/")[-1].removesuffix('.gz')
    raw = None
    for _ in range(trys):
        try:
            response = requests.get(url, stream=True)
            response.raise_for_status()
            raw = response.raw
            break
        except Exception:
            continue

    if not raw:
        log(f'Faild to download File: {csv_filename}')
        return
    
    with open(os.path.join(DOWNLOAD_FOLDER, csv_filename), 'wb') as csv_file:
        file_content = gzip.GzipFile(fileobj=raw) if url.endswith('.gz') else raw
        csv_file.write(file_content.read())

    return csv_filename


def list_website(url, trys = 5):
    """
    recursively finds all csv files and saves them to download_list.txt 
    """

    page = None

    for _ in range(trys):
        try:
            response = requests.get(url)
            response.raise_for_status()
            page = response.text
            break
        except Exception:
            continue

    if not page:
        log(f'Faild to list: {url}')
        return

    soup = BeautifulSoup(page, "html.parser")

    for item in reversed(soup.find_all("a")):
        link = item.get("href")
        if re.fullmatch(PATTERN_DAY, link):
            if not list_website(urljoin(url, link)):
                return False
        if re.fullmatch(PATTERN_YEAR, link):
            if not list_website(urljoin(url, link)):
                return False
        if link.endswith(".csv") or link.endswith(".gz"):
            for sensor_name in SensorModel._names.values():
                if sensor_name.lower() in link:
                    break
            else:
                continue
            csv_url = urljoin(url, link)
            if csv_url in all_csv_urls:
                return False
            print(csv_url, file=open(DOWNLOAD_LIST, 'a'))
    return True


def main():
    """
    1. update download list with all url newer than the newst in the download list
    2. download missing files
    """
    global all_csv_urls

    all_csv_urls = set(line.strip() for line in open(DOWNLOAD_LIST, "r").readlines())
    # append download list
    list_website(URL)

    # files that have already been downloaded
    cur_files = set(os.listdir(DOWNLOAD_FOLDER))

    # filter for location rather then device_id
    db = next(get_db())
    stations = set(str(s.device) for s in db.query(Station).all())

    urls_to_download = [url.strip() for url in open(DOWNLOAD_LIST, "r").readlines()]
    urls_to_download.sort(reverse=True)

    for url in tqdm(urls_to_download, desc="Downloading files", unit="files", file=open(PROGRESS_FILE, "w")):
        file_name = url.split("/")[-1]
        station_id = re.findall(PATTERN_STATION_ID, url)[0]

        if station_id not in stations:
            log(f"Skipp {station_id}")
            continue
        if file_name in cur_files:
            log(f"Already downloaded: {file_name}")
            break
        download(url)


if __name__ == '__main__':
    log_file = open(LOG_FILE, 'w')
    try:
        main()
    except Exception as e:
        import traceback
        s = traceback.format_exc()
        log(s)
    log_file.close()